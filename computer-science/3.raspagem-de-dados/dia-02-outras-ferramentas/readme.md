# Introdu√ß√£o
O que vamos aprender
Hoje daremos continuidade ao nosso aprendizado sobre raspagem de dados. Aprenderemos duas outras ferramentas utilizadas para web scraping: o Selenium e o Beautiful Soup. Veremos como fazer requisi√ß√µes web e como utiliz√°-las para extrair as informa√ß√µes que precisamos das respostas dessas requisi√ß√µes.

### Voc√™ ser√° capaz de

- Realizar requisi√ß√µes web;
- Analisar conte√∫dos HTML a fim de extrair dados;
- Utilizar a ferramenta Selenium para extrair dados;
- Utilizar a biblioteca Beautiful Soup para extrair dados;

## Por que isso √© importante?

Utilizar dados encontrados na web √© algo bem comum no cotidiano de pessoas desenvolvedoras. Contudo, esses dados nem sempre est√£o dispon√≠veis via API ou arquivos .csv. In√∫meras vezes as informa√ß√µes desejadas s√≥ est√£o dispon√≠veis como parte de uma p√°gina web, e nessas horas o web scraping √© extremamente √∫til em facilitar o acesso a essas informa√ß√µes.

Voc√™ j√° viu anteriormente como fazer o scraping de uma p√°gina utilizando a biblioteca requests em conjunto com a ferramenta parsel, mas como √© bem comum no mundo do desenvolvimento haver in√∫meras maneiras de se fazer algo, h√° uma boa variedade de ferramentas que podem te auxiliar a fazer raspagem de dados, cada uma com suas particularidades e pontos fortes.

Hoje voc√™ conhecer√° mais duas ferramentas para este prop√≥sito, para que possa aumentar seu repert√≥rio de como fazer raspagem em p√°ginas web, aprender a contornar alguns problemas e quem sabe at√© j√° eleger suas favoritas.

# Selenium

Essencialmente, o Selenium √© um conjunto de ferramentas para automa√ß√£o de navegadores da web, que permite controlar remotamente inst√¢ncias de navegadores e emular a intera√ß√£o de um usu√°rio com o navegador.

No n√∫cleo do Selenium est√° o WebDriver, uma API e protocolo que define uma interface para controlar o comportamento dos navegadores web. √â atrav√©s do WebDriver que o Selenium suporta a automa√ß√£o dos principais navegadores do mercado.

Cada navegador possui uma implementa√ß√£o espec√≠fica do WebDriver, chamada de driver, que √© o componente respons√°vel por delegar e manipular a comunica√ß√£o entre o Selenium e o navegador.

Para utilizar o Selenium √© necess√°rio instalar as bibliotecas de acordo com a linguagem que voc√™ utilizar√°, al√©m de ter instalado o navegador que deseja usar e o driver correspondente para ele.

Ap√≥s essas instala√ß√µes, com apenas algumas linhas de c√≥digo podemos criar uma inst√¢ncia de navegador para simular um acesso real a um site, como uma pessoa utilizando o browser faria.

Essa fun√ß√£o pode ser extremamente √∫til quando falamos de raspagem de dados, pois em Single Page Applications, por exemplo, como sites constru√≠dos em React, s√≥ ap√≥s uma requisi√ß√£o para a p√°gina √© que o conte√∫do ser√° montado e as informa√ß√µes estar√£o dispon√≠veis para serem consultadas. Simulando um acesso de uma pessoa real ao site, o Selenium pode evitar que o resultado da consulta volte vazio nesses casos.

Por exemplo, se voc√™ usar a biblioteca requests para fazer uma requisi√ß√£o do tipo get em um site feito em React, voc√™ vai receber de volta um HTML relativamente simples, somente com um elemento root e um c√≥digo JavaScript grande, que seria o respons√°vel por criar dinamicamente a p√°gina web. Acontece que esse c√≥digo JavaScript n√£o √© rodado pelo Python, permanecendo apenas um texto, fazendo com que sua p√°gina nunca seja montada e que voc√™ n√£o ache as informa√ß√µes que desejava. J√° o Selenium cria uma inst√¢ncia de um navegador, por exemplo o Firefox, e de fato executa o JavaScript como se fosse uma pessoa acessando o site pelo navegador normalmente, e somente ap√≥s o site terminar de carregar √© que o scrape √© feito.

## Instala√ß√£o do Selenium
Podemos utilizar o Selenium tanto instalado diretamente em nossa m√°quina, quanto atrav√©s de um container Docker. A seguir teremos o passo a passo das duas formas de instala√ß√£o da ferramenta, mas voc√™ precisa seguir apenas uma delas, a escolha fica inteiramente a seu crit√©rio, para conseguir executar o exemplo desenvolvido ao longo do conte√∫do.

Independentemente da forma escolhida, lembre-se de criar um ambiente virtual antes de instalar bibliotecas.

üëÄ De olho na dica: para recordar como criar um ambiente isolado, acesse este conte√∫do.

### Instala√ß√£o utilizando o Docker
Ao optar por essa instala√ß√£o, √© essencial ter o Docker instalado em seu computador. Para consultar como fazer a instala√ß√£o voc√™ pode acessar este conte√∫do.

A imagem que utilizaremos do Selenium √© a selenium/standalone-firefox:

````
docker pull selenium/standalone-firefox:106.0
````

### Iniciando a imagem Docker
Para efetivamente come√ßar a utilizar o Selenium, precisaremos inicializar a imagem Docker que baixamos anteriormente. Na documenta√ß√£o da imagem recomendada anteriormente √†s pessoas que utilizam Linux ou MacOS sem o novo processador M1, especificamente na se√ß√£o Using your browser (no VNC client is needed), que permite a inspe√ß√£o visual da atividade do container atrav√©s do navegador, encontramos os comandos que utilizaremos a seguir.

Faremos uma √∫nica adapta√ß√£o, que √© acrescentar a flag --name para nomear o container e facilitar sua manipula√ß√£o.

````
docker run -d -p 4444:4444 -p 7900:7900 --shm-size 2g --name firefox selenium/standalone-firefox:106.0
````
As flags utilizadas acima t√™m as seguintes finalidades:

- -d serve para rodar o container em segundo plano
- -p vincula uma porta do SO a outra porta dentro do container
- --shm-size aumenta o limite de quantidade de mem√≥ria compartilhada com o container
- --name define qual vai ser o nome do container
- --platform (somente se utilizado) diz ao docker qual a plataforma deve ser usada

## Primeiros passos com o Selenium
Vamos criar um arquivo para fazer nossos primeiros experimentos com o Selenium! üöÄ

üëÄ De olho na dica: √© importante evitar utilizar os nomes das bibliotecas e ferramentas para nomear os arquivos.

‚ö†Ô∏è Para que o c√≥digo do exemplo funcione, lembre-se que √© necess√°rio ter o Firefox instalado no seu computador.

Agora vamos colocar a m√£o na massa! Crie o seguinte arquivo:

````
# importa√ß√£o do webdriver, que √© o que possibilita a implementa√ß√£o para todos
# os principais navegadores da web
from selenium import webdriver

# Para usar o chrome ao inv√©s do firefox trocamos FirefoxOptions por ChromeOptions
# Todavia, caso esteja utilizando o docker, atente-se ao container sendo utilizado.
options = webdriver.FirefoxOptions()
options.add_argument('--ignore-certificate-errors')
options.add_argument('--ignore-ssl-errors=yes')
options.add_argument('--start-maximized')

firefox = webdriver.Remote(command_executor="http://localhost:4444/wd/hub", options=options)

# requisi√ß√µes para essa inst√¢ncia criada utilizando o m√©todo `get`
response = firefox.get("https://www.python.org/")

# espera 10 segundos
sleep(10)
# encerra o navegador, importante quando usamos containers
firefox.quit()
````
Daqui para frente, utilizaremos como base o c√≥digo em que a instala√ß√£o foi feita diretamente na m√°quina, por isso tenha aten√ß√£o nas linhas que trazem as options e a que faz a defini√ß√£o de firefox, pois elas n√£o poder√£o ser removidas no caso de voc√™ estar utilizando Docker. O restante do c√≥digo pode seguir as instru√ß√µes dos pr√≥ximos passos normalmente.

Voc√™ pode instruir o navegador a realizar diversas opera√ß√µes atrav√©s do c√≥digo. Para dar um exemplo e evidenciar o potencial do Selenium, com apenas tr√™s linhas conseguimos fazer com que logo ap√≥s abrir o navegador, algo seja digitado no campo de pesquisa e o selenium pressione enter para efetivar a busca:

````
# from selenium import webdriver
from selenium.webdriver.common.keys import Keys # Importa teclas comuns

# firefox = webdriver.Firefox()

response = firefox.get("https://www.google.com.br/")

# pesquisa na inst√¢ncia aberta do navegador pela primeira ocorr√™ncia
# do nome 'q'
search_input = firefox.find_element("name", "q")

# escreve selenium dentro do campo de pesquisa
search_input.send_keys("selenium")

# pressiona enter para realizar a busca
search_input.send_keys(Keys.ENTER)
````

Partindo para a parte que nos interessa, a de web scraping, vamos juntar os conhecimentos da √∫ltima aula ao que j√° vimos hoje e pegar algumas informa√ß√µes dos livros de uma p√°gina dedicada para treinar scraping.

 Selenium tem v√°rios m√©todos p√∫blicos, como o find_element que usamos anteriormente e o find_elements (no plural), utilizados para localizar o primeiro elemento correspondente ao resultado do filtro ou todos os elementos que se encaixarem na busca, respectivamente.

Tamb√©m podemos utilizar o By para especificar um elemento CSS ou XPATH que ser√° buscado, para isso √© necess√°rio import√°-lo via:

`from selenium.webdriver.common.by import By`

Nesse caso, devemos passar dois par√¢metros: o primeiro par√¢metro define o que voc√™ ir√° buscar e o segundo o filtro da nossa pesquisa. Abaixo temos duas op√ß√µes no que diz respeito ao que estamos buscando, uma delas utilizando o By e a outra no formato previamente utilizado (quando buscamos pelo campo de nome q no exemplo acima).


| Com o By | Sem o By|
| -------- | -------- |
| By.ID | ‚Äúid‚Äù |
| By.NAME | ‚Äúname‚Äù |
| By.XPATH | ‚Äúxpath‚Äù |
| By.LINK_TEXT| ‚Äúlink text‚Äù |
| By.PARTIAL_LINK_TEXT| ‚Äúpartial link text‚Äù |
| By.TAG_NAME| ‚Äútag name‚Äù |
| By.CLASS_NAME| ‚Äúclass name‚Äù|
| By.CSS_SELECTOR| 	‚Äúcss selector‚Äù |

Assim como quando utilizamos a lib requests, inspecionar a p√°gina que queremos raspar √© imprescind√≠vel para entendermos a estrutura de como a p√°gina foi montada e quais elementos devemos selecionar para buscar as informa√ß√µes que queremos.

````
from selenium import webdriver

# Importa o By
from selenium.webdriver.common.by import By

firefox = webdriver.Firefox()

firefox.get("https://books.toscrape.com/")


# Define a fun√ß√£o que far√° o scrape da URL recebida
def scrape(url):
    firefox.get(url)

    # Itera entre os elementos com essa classe
    for book in firefox.find_elements(By.CLASS_NAME, "product_pod"):
        # Cria dict vazio para guardar os elementos capturados
        new_item = {}

        # Cria uma chave 'title' no dict que vai receber o resultado da busca
        # O t√≠tulo est√° em uma tag anchor que est√° dentro de uma tag 'H3'
        new_item["title"] = (
            book.find_element(By.TAG_NAME, "h3")
            .find_element(By.TAG_NAME, "a")
            .get_attribute("innerHTML")
        )

        # O trecho do book est√° em um elemento da classe 'price_color'
        new_item["price"] = book.find_element(
            By.CLASS_NAME, "price_color"
        ).get_attribute("innerHTML")

        # O link est√° dentro de um atributo 'href'
        new_item["link"] = (
            book.find_element(By.CLASS_NAME, "image_container")
            .find_element(By.TAG_NAME, "a")
            .get_attribute("href")
        )

        print(new_item)


scrape("https://books.toscrape.com/")
````

Utilizamos bastante no c√≥digo acima o m√©todo get_attribute. A raz√£o disso √© que com o Selenium, o elemento retornado depois da busca pelo atributo CSS ser√° do tipo webdriver e n√£o texto. √â justamente para fazer essa convers√£o que utilizamos esse m√©todo especificando o atributo ‚ÄòinnerHTML‚Äô ou ‚Äòhref‚Äô, por exemplo.

Tamb√©m utilizamos o m√©todo find_element encadeado para procurar um elemento dentro de outro elemento, como fizemos para pegar o link, por exemplo, que era um elemento √¢ncora dentro de uma div.

Certo, mas at√© agora s√≥ pegamos informa√ß√µes dos livros da primeira p√°gina do site, como fazemos para pegar todos os livros, at√© a √∫ltima p√°gina? ü§î

- Primeiro precisamos organizar nosso c√≥digo e determinar que o retorno da fun√ß√£o scrape salve o resultado da raspagem em uma lista.
- Em seguida, temos que criar uma nova lista para abrigar os dados de uma p√°gina;
- Depois devemos acessar o bot√£o para ir para a pr√≥xima p√°gina e l√° refazer o processo de salvar todas as informa√ß√µes solicitadas, repetindo esse mecanismo at√© todas as p√°ginas serem percorridas.

Vamos ver como fazer isso com Selenium?

````
# from selenium import webdriver

# from selenium.webdriver.common.by import By


# firefox = webdriver.Firefox()


# def scrape(url):
#     firefox.get(url)

    books = []

#     for book in firefox.find_elements(By.CLASS_NAME, "product_pod"):
#         new_item = {}

#         new_item["title"] = (
#             book.find_element(By.TAG_NAME, "h3")
#             .find_element(By.TAG_NAME, "a")
#             .get_attribute("innerHTML")
#         )

#         new_item["price"] = book.find_element(
#             By.CLASS_NAME, "price_color"
#         ).get_attribute("innerHTML")

#         new_item["link"] = (
#             book.find_element(By.CLASS_NAME, "image_container")
#             .find_element(By.TAG_NAME, "a")
#             .get_attribute("href")
#         )

        books.append(new_item)
    return books

firefox.get("https://books.toscrape.com/")

all_books = []
url2request = "https://books.toscrape.com/"

# Cria uma vari√°vel com o seletor que captura o link do bot√£o de passar para
# a pr√≥xima p√°gina
next_page_link = (
    firefox.find_element(By.CLASS_NAME, "next")
    .get_attribute("innerHTML")
)

# Enquanto este bot√£o de 'next' existir na p√°gina ele ir√° iterar
while next_page_link:

    # Usa o m√©todo extend para colocar todos os elementos de uma lista dentro
    # de outra
    all_books.extend(scrape(url2request))
    url2request = (
        firefox.find_element(By.CLASS_NAME, "next")
        .find_element(By.TAG_NAME, "a")
        .get_attribute("href")
    )

print(all_books)
````

Observando o navegador aberto com a execu√ß√£o do c√≥digo, voc√™ ver√° que ele abriu na p√°gina solicitada e em seguida come√ßou a percorrer as p√°ginas do site, o que indica que tudo est√° correndo bem. Contudo, pouco antes de finalizar, j√° na √∫ltima p√°gina, no terminal aparece uma exce√ß√£o do Selenium chamada ‚ÄòNoSuchElementException‚Äô.

Pela mensagem fica mais f√°cil de entender o que deu errado. Ao entrar na √∫ltima p√°gina, ele fez a raspagem de todas as informa√ß√µes pedidas, por√©m o passo seguinte foi imposs√≠vel de executar j√° que precisava achar o bot√£o ‚Äònext‚Äô para alterar o link na vari√°vel url2request. A forma mais simples de resolver este erro √© importar a exce√ß√£o do Selenium e trat√°-la com um try, de forma que ao ocorrer esta situa√ß√£o o loop seja quebrado.

````
# from selenium import webdriver

# from selenium.webdriver.common.by import By

# from selenium.common.exceptions import NoSuchElementException

# firefox = webdriver.Firefox()


# def scrape(url):
#     firefox.get(url)

#     books = []

#     for book in firefox.find_elements(By.CLASS_NAME, "product_pod"):
#         new_item = {}

#         new_item["title"] = (
#             book.find_element(By.TAG_NAME, "h3")
#             .find_element(By.TAG_NAME, "a")
#             .get_attribute("innerHTML")
#         )

#         new_item["price"] = book.find_element(
#             By.CLASS_NAME, "price_color"
#         ).get_attribute("innerHTML")

#         new_item["link"] = (
#             book.find_element(By.CLASS_NAME, "image_container")
#             .find_element(By.TAG_NAME, "a")
#             .get_attribute("href")
#         )

#         books.append(new_item)
#     return books


# firefox.get("https://books.toscrape.com/")

# all_books = []
# url2request = "https://books.toscrape.com/"

# next_page_link = (
#     firefox.find_element(By.CLASS_NAME, "next")
#     .get_attribute("innerHTML")
# )

# while next_page_link:

#     all_books.extend(scrape(url2request))
    try:
        url2request = (
            firefox.find_element(By.CLASS_NAME, 'next')
            .find_element(By.TAG_NAME, 'a').get_attribute('href')
        )
    except NoSuchElementException:
        print("exception handled")
        break

# print(all_books)
````

Agora sim, ap√≥s uma eternidade percorrer todas as p√°ginas do site temos as informa√ß√µes que queremos de todos os 1000 livros l√° existentes! ü•≥

Antes de partirmos para a pr√≥xima ferramenta que veremos hoje, aqui vai uma √∫ltima dica: √© muito legal ver o navegador abrir e executar os comandos que definimos, por√©m quando n√£o precisamos ou n√£o queremos ver essa execu√ß√£o, podemos evit√°-la utilizando as options. Basta import√°-las do webdriver, adicionar um novo argumento com a op√ß√£o que deseja e depois pass√°-la como par√¢metro para a inst√¢ncia de navegador criada.

üí° Caso queira explorar outras possibilidades oferecidas pelas options, voc√™ pode consultar este link da documenta√ß√£o. Ele redireciona para as op√ß√µes dispon√≠veis para o Firefox, mas na mesma p√°gina voc√™ encontra menus para consultar sobre outros navegadores.

# Beautiful Soup

Beautiful Soup √© uma biblioteca Python para extrair dados de arquivos HTML e XML. Ela √© de grande valia para analisar esses arquivos e fornecer maneiras mais simples de navegar, pesquisar e modificar a √°rvore de an√°lise, podendo economizar v√°rias horas de trabalho.

Veremos como utilizar alguns dos recursos do Beautiful Soup 4, mas antes de analisar as informa√ß√µes precisamos baixar o conte√∫do da p√°gina que queremos utilizar.

## Primeiros passos
Com base no que voc√™ j√° sabe sobre web scraping, j√° pode ter uma ideia de ferramentas que podemos usar para fazer a requisi√ß√£o para a p√°gina e baixar seu conte√∫do HTML.

Para nosso exemplo, usaremos a biblioteca requests(especificamente na vers√£o 2.27.1) e faremos uma requisi√ß√£o do tipo get para uma p√°gina de frases.

````
import requests

url = "https://quotes.toscrape.com"
page = requests.get(url)
print(page.content)
````
Com o conte√∫do da p√°gina baixado, vamos ao que interessa, ver como utilizar o Beautiful Soup para fazer sua an√°lise!

## Instala√ß√£o das bibliotecas
‚ö†Ô∏è Lembre-se de estar em um ambiente virtual antes de fazer as instala√ß√µes de bibliotecas.

Para instalar as bibliotecas Beautiful Soup e requests basta digitar em seu terminal:

`pip3 install beautifulsoup4==4.11.1 requests==2.27.1`

Agora podemos importar a lib em nosso arquivo e come√ßar a explorar as funcionalidades e facilidades que a ferramenta oferece.

````
# import requests
from bs4 import BeautifulSoup

# url = "https://quotes.toscrape.com"
# page = requests.get(url)
html_content = page.text

# Cria uma inst√¢ncia do objeto Beautiful Soup e usa o parser de HTML nativo
# do Python
soup = BeautifulSoup(html_content, "html.parser")

# Utiliza o m√©todo prettify para melhorar a visualiza√ß√£o do conte√∫do
print(soup.prettify())
````