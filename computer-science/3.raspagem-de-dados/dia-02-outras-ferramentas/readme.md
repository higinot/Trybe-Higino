# Introdu√ß√£o
O que vamos aprender
Hoje daremos continuidade ao nosso aprendizado sobre raspagem de dados. Aprenderemos duas outras ferramentas utilizadas para web scraping: o Selenium e o Beautiful Soup. Veremos como fazer requisi√ß√µes web e como utiliz√°-las para extrair as informa√ß√µes que precisamos das respostas dessas requisi√ß√µes.

### Voc√™ ser√° capaz de

- Realizar requisi√ß√µes web;
- Analisar conte√∫dos HTML a fim de extrair dados;
- Utilizar a ferramenta Selenium para extrair dados;
- Utilizar a biblioteca Beautiful Soup para extrair dados;

## Por que isso √© importante?

Utilizar dados encontrados na web √© algo bem comum no cotidiano de pessoas desenvolvedoras. Contudo, esses dados nem sempre est√£o dispon√≠veis via API ou arquivos .csv. In√∫meras vezes as informa√ß√µes desejadas s√≥ est√£o dispon√≠veis como parte de uma p√°gina web, e nessas horas o web scraping √© extremamente √∫til em facilitar o acesso a essas informa√ß√µes.

Voc√™ j√° viu anteriormente como fazer o scraping de uma p√°gina utilizando a biblioteca requests em conjunto com a ferramenta parsel, mas como √© bem comum no mundo do desenvolvimento haver in√∫meras maneiras de se fazer algo, h√° uma boa variedade de ferramentas que podem te auxiliar a fazer raspagem de dados, cada uma com suas particularidades e pontos fortes.

Hoje voc√™ conhecer√° mais duas ferramentas para este prop√≥sito, para que possa aumentar seu repert√≥rio de como fazer raspagem em p√°ginas web, aprender a contornar alguns problemas e quem sabe at√© j√° eleger suas favoritas.

# Selenium

Essencialmente, o Selenium √© um conjunto de ferramentas para automa√ß√£o de navegadores da web, que permite controlar remotamente inst√¢ncias de navegadores e emular a intera√ß√£o de um usu√°rio com o navegador.

No n√∫cleo do Selenium est√° o WebDriver, uma API e protocolo que define uma interface para controlar o comportamento dos navegadores web. √â atrav√©s do WebDriver que o Selenium suporta a automa√ß√£o dos principais navegadores do mercado.

Cada navegador possui uma implementa√ß√£o espec√≠fica do WebDriver, chamada de driver, que √© o componente respons√°vel por delegar e manipular a comunica√ß√£o entre o Selenium e o navegador.

Para utilizar o Selenium √© necess√°rio instalar as bibliotecas de acordo com a linguagem que voc√™ utilizar√°, al√©m de ter instalado o navegador que deseja usar e o driver correspondente para ele.

Ap√≥s essas instala√ß√µes, com apenas algumas linhas de c√≥digo podemos criar uma inst√¢ncia de navegador para simular um acesso real a um site, como uma pessoa utilizando o browser faria.

Essa fun√ß√£o pode ser extremamente √∫til quando falamos de raspagem de dados, pois em Single Page Applications, por exemplo, como sites constru√≠dos em React, s√≥ ap√≥s uma requisi√ß√£o para a p√°gina √© que o conte√∫do ser√° montado e as informa√ß√µes estar√£o dispon√≠veis para serem consultadas. Simulando um acesso de uma pessoa real ao site, o Selenium pode evitar que o resultado da consulta volte vazio nesses casos.

Por exemplo, se voc√™ usar a biblioteca requests para fazer uma requisi√ß√£o do tipo get em um site feito em React, voc√™ vai receber de volta um HTML relativamente simples, somente com um elemento root e um c√≥digo JavaScript grande, que seria o respons√°vel por criar dinamicamente a p√°gina web. Acontece que esse c√≥digo JavaScript n√£o √© rodado pelo Python, permanecendo apenas um texto, fazendo com que sua p√°gina nunca seja montada e que voc√™ n√£o ache as informa√ß√µes que desejava. J√° o Selenium cria uma inst√¢ncia de um navegador, por exemplo o Firefox, e de fato executa o JavaScript como se fosse uma pessoa acessando o site pelo navegador normalmente, e somente ap√≥s o site terminar de carregar √© que o scrape √© feito.

## Instala√ß√£o do Selenium
Podemos utilizar o Selenium tanto instalado diretamente em nossa m√°quina, quanto atrav√©s de um container Docker. A seguir teremos o passo a passo das duas formas de instala√ß√£o da ferramenta, mas voc√™ precisa seguir apenas uma delas, a escolha fica inteiramente a seu crit√©rio, para conseguir executar o exemplo desenvolvido ao longo do conte√∫do.

Independentemente da forma escolhida, lembre-se de criar um ambiente virtual antes de instalar bibliotecas.

üëÄ De olho na dica: para recordar como criar um ambiente isolado, acesse este conte√∫do.

### Instala√ß√£o utilizando o Docker
Ao optar por essa instala√ß√£o, √© essencial ter o Docker instalado em seu computador. Para consultar como fazer a instala√ß√£o voc√™ pode acessar este conte√∫do.

A imagem que utilizaremos do Selenium √© a selenium/standalone-firefox:

````
docker pull selenium/standalone-firefox:106.0
````

### Iniciando a imagem Docker
Para efetivamente come√ßar a utilizar o Selenium, precisaremos inicializar a imagem Docker que baixamos anteriormente. Na documenta√ß√£o da imagem recomendada anteriormente √†s pessoas que utilizam Linux ou MacOS sem o novo processador M1, especificamente na se√ß√£o Using your browser (no VNC client is needed), que permite a inspe√ß√£o visual da atividade do container atrav√©s do navegador, encontramos os comandos que utilizaremos a seguir.

Faremos uma √∫nica adapta√ß√£o, que √© acrescentar a flag --name para nomear o container e facilitar sua manipula√ß√£o.

````
docker run -d -p 4444:4444 -p 7900:7900 --shm-size 2g --name firefox selenium/standalone-firefox:106.0
````
As flags utilizadas acima t√™m as seguintes finalidades:

- -d serve para rodar o container em segundo plano
- -p vincula uma porta do SO a outra porta dentro do container
- --shm-size aumenta o limite de quantidade de mem√≥ria compartilhada com o container
- --name define qual vai ser o nome do container
- --platform (somente se utilizado) diz ao docker qual a plataforma deve ser usada

## Primeiros passos com o Selenium
Vamos criar um arquivo para fazer nossos primeiros experimentos com o Selenium! üöÄ

üëÄ De olho na dica: √© importante evitar utilizar os nomes das bibliotecas e ferramentas para nomear os arquivos.

‚ö†Ô∏è Para que o c√≥digo do exemplo funcione, lembre-se que √© necess√°rio ter o Firefox instalado no seu computador.

Agora vamos colocar a m√£o na massa! Crie o seguinte arquivo:

````
# importa√ß√£o do webdriver, que √© o que possibilita a implementa√ß√£o para todos
# os principais navegadores da web
from selenium import webdriver

# Para usar o chrome ao inv√©s do firefox trocamos FirefoxOptions por ChromeOptions
# Todavia, caso esteja utilizando o docker, atente-se ao container sendo utilizado.
options = webdriver.FirefoxOptions()
options.add_argument('--ignore-certificate-errors')
options.add_argument('--ignore-ssl-errors=yes')
options.add_argument('--start-maximized')

firefox = webdriver.Remote(command_executor="http://localhost:4444/wd/hub", options=options)

# requisi√ß√µes para essa inst√¢ncia criada utilizando o m√©todo `get`
response = firefox.get("https://www.python.org/")

# espera 10 segundos
sleep(10)
# encerra o navegador, importante quando usamos containers
firefox.quit()
````
Daqui para frente, utilizaremos como base o c√≥digo em que a instala√ß√£o foi feita diretamente na m√°quina, por isso tenha aten√ß√£o nas linhas que trazem as options e a que faz a defini√ß√£o de firefox, pois elas n√£o poder√£o ser removidas no caso de voc√™ estar utilizando Docker. O restante do c√≥digo pode seguir as instru√ß√µes dos pr√≥ximos passos normalmente.

Voc√™ pode instruir o navegador a realizar diversas opera√ß√µes atrav√©s do c√≥digo. Para dar um exemplo e evidenciar o potencial do Selenium, com apenas tr√™s linhas conseguimos fazer com que logo ap√≥s abrir o navegador, algo seja digitado no campo de pesquisa e o selenium pressione enter para efetivar a busca:

````
# from selenium import webdriver
from selenium.webdriver.common.keys import Keys # Importa teclas comuns

# firefox = webdriver.Firefox()

response = firefox.get("https://www.google.com.br/")

# pesquisa na inst√¢ncia aberta do navegador pela primeira ocorr√™ncia
# do nome 'q'
search_input = firefox.find_element("name", "q")

# escreve selenium dentro do campo de pesquisa
search_input.send_keys("selenium")

# pressiona enter para realizar a busca
search_input.send_keys(Keys.ENTER)
````

Partindo para a parte que nos interessa, a de web scraping, vamos juntar os conhecimentos da √∫ltima aula ao que j√° vimos hoje e pegar algumas informa√ß√µes dos livros de uma p√°gina dedicada para treinar scraping.

 Selenium tem v√°rios m√©todos p√∫blicos, como o find_element que usamos anteriormente e o find_elements (no plural), utilizados para localizar o primeiro elemento correspondente ao resultado do filtro ou todos os elementos que se encaixarem na busca, respectivamente.

Tamb√©m podemos utilizar o By para especificar um elemento CSS ou XPATH que ser√° buscado, para isso √© necess√°rio import√°-lo via:

`from selenium.webdriver.common.by import By`

Nesse caso, devemos passar dois par√¢metros: o primeiro par√¢metro define o que voc√™ ir√° buscar e o segundo o filtro da nossa pesquisa. Abaixo temos duas op√ß√µes no que diz respeito ao que estamos buscando, uma delas utilizando o By e a outra no formato previamente utilizado (quando buscamos pelo campo de nome q no exemplo acima).


| Com o By | Sem o By|
| -------- | -------- |
| By.ID | ‚Äúid‚Äù |
| By.NAME | ‚Äúname‚Äù |
| By.XPATH | ‚Äúxpath‚Äù |
| By.LINK_TEXT| ‚Äúlink text‚Äù |
| By.PARTIAL_LINK_TEXT| ‚Äúpartial link text‚Äù |
| By.TAG_NAME| ‚Äútag name‚Äù |
| By.CLASS_NAME| ‚Äúclass name‚Äù|
| By.CSS_SELECTOR| 	‚Äúcss selector‚Äù |

Assim como quando utilizamos a lib requests, inspecionar a p√°gina que queremos raspar √© imprescind√≠vel para entendermos a estrutura de como a p√°gina foi montada e quais elementos devemos selecionar para buscar as informa√ß√µes que queremos.

````
from selenium import webdriver

# Importa o By
from selenium.webdriver.common.by import By

firefox = webdriver.Firefox()

firefox.get("https://books.toscrape.com/")


# Define a fun√ß√£o que far√° o scrape da URL recebida
def scrape(url):
    firefox.get(url)

    # Itera entre os elementos com essa classe
    for book in firefox.find_elements(By.CLASS_NAME, "product_pod"):
        # Cria dict vazio para guardar os elementos capturados
        new_item = {}

        # Cria uma chave 'title' no dict que vai receber o resultado da busca
        # O t√≠tulo est√° em uma tag anchor que est√° dentro de uma tag 'H3'
        new_item["title"] = (
            book.find_element(By.TAG_NAME, "h3")
            .find_element(By.TAG_NAME, "a")
            .get_attribute("innerHTML")
        )

        # O trecho do book est√° em um elemento da classe 'price_color'
        new_item["price"] = book.find_element(
            By.CLASS_NAME, "price_color"
        ).get_attribute("innerHTML")

        # O link est√° dentro de um atributo 'href'
        new_item["link"] = (
            book.find_element(By.CLASS_NAME, "image_container")
            .find_element(By.TAG_NAME, "a")
            .get_attribute("href")
        )

        print(new_item)


scrape("https://books.toscrape.com/")
````

Utilizamos bastante no c√≥digo acima o m√©todo get_attribute. A raz√£o disso √© que com o Selenium, o elemento retornado depois da busca pelo atributo CSS ser√° do tipo webdriver e n√£o texto. √â justamente para fazer essa convers√£o que utilizamos esse m√©todo especificando o atributo ‚ÄòinnerHTML‚Äô ou ‚Äòhref‚Äô, por exemplo.

Tamb√©m utilizamos o m√©todo find_element encadeado para procurar um elemento dentro de outro elemento, como fizemos para pegar o link, por exemplo, que era um elemento √¢ncora dentro de uma div.

Certo, mas at√© agora s√≥ pegamos informa√ß√µes dos livros da primeira p√°gina do site, como fazemos para pegar todos os livros, at√© a √∫ltima p√°gina? ü§î

- Primeiro precisamos organizar nosso c√≥digo e determinar que o retorno da fun√ß√£o scrape salve o resultado da raspagem em uma lista.
- Em seguida, temos que criar uma nova lista para abrigar os dados de uma p√°gina;
- Depois devemos acessar o bot√£o para ir para a pr√≥xima p√°gina e l√° refazer o processo de salvar todas as informa√ß√µes solicitadas, repetindo esse mecanismo at√© todas as p√°ginas serem percorridas.

Vamos ver como fazer isso com Selenium?

````
# from selenium import webdriver

# from selenium.webdriver.common.by import By


# firefox = webdriver.Firefox()


# def scrape(url):
#     firefox.get(url)

    books = []

#     for book in firefox.find_elements(By.CLASS_NAME, "product_pod"):
#         new_item = {}

#         new_item["title"] = (
#             book.find_element(By.TAG_NAME, "h3")
#             .find_element(By.TAG_NAME, "a")
#             .get_attribute("innerHTML")
#         )

#         new_item["price"] = book.find_element(
#             By.CLASS_NAME, "price_color"
#         ).get_attribute("innerHTML")

#         new_item["link"] = (
#             book.find_element(By.CLASS_NAME, "image_container")
#             .find_element(By.TAG_NAME, "a")
#             .get_attribute("href")
#         )

        books.append(new_item)
    return books

firefox.get("https://books.toscrape.com/")

all_books = []
url2request = "https://books.toscrape.com/"

# Cria uma vari√°vel com o seletor que captura o link do bot√£o de passar para
# a pr√≥xima p√°gina
next_page_link = (
    firefox.find_element(By.CLASS_NAME, "next")
    .get_attribute("innerHTML")
)

# Enquanto este bot√£o de 'next' existir na p√°gina ele ir√° iterar
while next_page_link:

    # Usa o m√©todo extend para colocar todos os elementos de uma lista dentro
    # de outra
    all_books.extend(scrape(url2request))
    url2request = (
        firefox.find_element(By.CLASS_NAME, "next")
        .find_element(By.TAG_NAME, "a")
        .get_attribute("href")
    )

print(all_books)
````

Observando o navegador aberto com a execu√ß√£o do c√≥digo, voc√™ ver√° que ele abriu na p√°gina solicitada e em seguida come√ßou a percorrer as p√°ginas do site, o que indica que tudo est√° correndo bem. Contudo, pouco antes de finalizar, j√° na √∫ltima p√°gina, no terminal aparece uma exce√ß√£o do Selenium chamada ‚ÄòNoSuchElementException‚Äô.

Pela mensagem fica mais f√°cil de entender o que deu errado. Ao entrar na √∫ltima p√°gina, ele fez a raspagem de todas as informa√ß√µes pedidas, por√©m o passo seguinte foi imposs√≠vel de executar j√° que precisava achar o bot√£o ‚Äònext‚Äô para alterar o link na vari√°vel url2request. A forma mais simples de resolver este erro √© importar a exce√ß√£o do Selenium e trat√°-la com um try, de forma que ao ocorrer esta situa√ß√£o o loop seja quebrado.

````
# from selenium import webdriver

# from selenium.webdriver.common.by import By

# from selenium.common.exceptions import NoSuchElementException

# firefox = webdriver.Firefox()


# def scrape(url):
#     firefox.get(url)

#     books = []

#     for book in firefox.find_elements(By.CLASS_NAME, "product_pod"):
#         new_item = {}

#         new_item["title"] = (
#             book.find_element(By.TAG_NAME, "h3")
#             .find_element(By.TAG_NAME, "a")
#             .get_attribute("innerHTML")
#         )

#         new_item["price"] = book.find_element(
#             By.CLASS_NAME, "price_color"
#         ).get_attribute("innerHTML")

#         new_item["link"] = (
#             book.find_element(By.CLASS_NAME, "image_container")
#             .find_element(By.TAG_NAME, "a")
#             .get_attribute("href")
#         )

#         books.append(new_item)
#     return books


# firefox.get("https://books.toscrape.com/")

# all_books = []
# url2request = "https://books.toscrape.com/"

# next_page_link = (
#     firefox.find_element(By.CLASS_NAME, "next")
#     .get_attribute("innerHTML")
# )

# while next_page_link:

#     all_books.extend(scrape(url2request))
    try:
        url2request = (
            firefox.find_element(By.CLASS_NAME, 'next')
            .find_element(By.TAG_NAME, 'a').get_attribute('href')
        )
    except NoSuchElementException:
        print("exception handled")
        break

# print(all_books)
````

Agora sim, ap√≥s uma eternidade percorrer todas as p√°ginas do site temos as informa√ß√µes que queremos de todos os 1000 livros l√° existentes! ü•≥

Antes de partirmos para a pr√≥xima ferramenta que veremos hoje, aqui vai uma √∫ltima dica: √© muito legal ver o navegador abrir e executar os comandos que definimos, por√©m quando n√£o precisamos ou n√£o queremos ver essa execu√ß√£o, podemos evit√°-la utilizando as options. Basta import√°-las do webdriver, adicionar um novo argumento com a op√ß√£o que deseja e depois pass√°-la como par√¢metro para a inst√¢ncia de navegador criada.

üí° Caso queira explorar outras possibilidades oferecidas pelas options, voc√™ pode consultar este link da documenta√ß√£o. Ele redireciona para as op√ß√µes dispon√≠veis para o Firefox, mas na mesma p√°gina voc√™ encontra menus para consultar sobre outros navegadores.

# Beautiful Soup

Beautiful Soup √© uma biblioteca Python para extrair dados de arquivos HTML e XML. Ela √© de grande valia para analisar esses arquivos e fornecer maneiras mais simples de navegar, pesquisar e modificar a √°rvore de an√°lise, podendo economizar v√°rias horas de trabalho.

Veremos como utilizar alguns dos recursos do Beautiful Soup 4, mas antes de analisar as informa√ß√µes precisamos baixar o conte√∫do da p√°gina que queremos utilizar.

## Primeiros passos
Com base no que voc√™ j√° sabe sobre web scraping, j√° pode ter uma ideia de ferramentas que podemos usar para fazer a requisi√ß√£o para a p√°gina e baixar seu conte√∫do HTML.

Para nosso exemplo, usaremos a biblioteca requests(especificamente na vers√£o 2.27.1) e faremos uma requisi√ß√£o do tipo get para uma p√°gina de frases.

````
import requests

url = "https://quotes.toscrape.com"
page = requests.get(url)
print(page.content)
````
Com o conte√∫do da p√°gina baixado, vamos ao que interessa, ver como utilizar o Beautiful Soup para fazer sua an√°lise!

## Instala√ß√£o das bibliotecas
‚ö†Ô∏è Lembre-se de estar em um ambiente virtual antes de fazer as instala√ß√µes de bibliotecas.

Para instalar as bibliotecas Beautiful Soup e requests basta digitar em seu terminal:

`pip3 install beautifulsoup4==4.11.1 requests==2.27.1`

Agora podemos importar a lib em nosso arquivo e come√ßar a explorar as funcionalidades e facilidades que a ferramenta oferece.

````
# import requests
from bs4 import BeautifulSoup

# url = "https://quotes.toscrape.com"
# page = requests.get(url)
html_content = page.text

# Cria uma inst√¢ncia do objeto Beautiful Soup e usa o parser de HTML nativo
# do Python
soup = BeautifulSoup(html_content, "html.parser")

# Utiliza o m√©todo prettify para melhorar a visualiza√ß√£o do conte√∫do
print(soup.prettify())
````

O retorno desse c√≥digo √© o HTML da p√°gina, j√° formatado de uma forma muito amig√°vel para a leitura, n√£o concorda?

## Tipos de objetos do Beautiful Soup
O Beautiful Soup transforma um documento HTML complexo em uma √°rvore de objetos Python. Os quatro tipos de objetos que podemos lidar s√£o Tag, NavigableString, BeautifulSoup e Comment. Na documenta√ß√£o, que est√° dispon√≠vel inclusive em portugu√™s, existe uma se√ß√£o inteira dedicada aos tipos de objetos, mas destacaremos aqui apenas os dois primeiros.

## Tag
Em suma, um objeto do tipo Tag corresponde a uma tag XML ou HTML do documento original. Toda tag possui um nome acess√≠vel atrav√©s de .name. Por exemplo, quando vemos header, ele √© um elemento do tipo tag e o nome dessa tag √© header.

As tags tamb√©m podem ter atributos, como classes, ids e etc. Esses atributos s√£o acess√≠veis considerando tag como um dicion√°rio e como podem receber m√∫ltiplos valores, s√£o apresentados em forma de lista.

````
# import requests
# from bs4 import BeautifulSoup

# url = "https://quotes.toscrape.com"
# page = requests.get(url)
# html_content = page.text

# soup = BeautifulSoup(html_content, "html.parser")


# acessando a tag 'title'
title = soup.title

# retorna o elemento HTML da tag
print(title)

# o tipo de 'title' √© tag
print(type(title))

# o nome de 'title' √© title
print(title.name)

# acessando a tag 'footer'
footer = soup.footer

# acessando o atributo classe da tag footer
print(footer['class'])
````

## NavigableString
Uma string corresponde a um texto dentro de uma tag e esse texto fica armazenado na classe NavigableString.

````
# import requests
# from bs4 import BeautifulSoup

# url = "https://quotes.toscrape.com"
# page = requests.get(url)
# html_content = page.text

# soup = BeautifulSoup(html_content, "html.parser")

# title = soup.title
# footer = soup.footer

# retorna o elemento HTML da tag
print(title)

# Acessando a string de uma tag
print(title.string)

# Verificando o tipo dessa string
print(type(title.string))
````

## Buscando na √°rvore
Assim como nas outras ferramentas apresentadas at√© aqui, o Beautiful Soup tamb√©m possui dois m√©todos principais para encontrar elementos. Eles s√£o o find() e find_all() e a essa altura voc√™ j√° deve ter presumido que a diferen√ßa b√°sica entre eles √© que o primeiro retorna apenas o primeiro elemento que corresponder ao filtro, enquanto o segundo retorna a lista de todos os elementos que baterem com o filtro.

H√° v√°rias possibilidades de filtros a serem utilizados dentro dos m√©todos descritos acima, de strings e regex, at√© fun√ß√µes, e ler a documenta√ß√£o √© essencial para garantir que voc√™ est√° utilizando o m√©todo mais adequado para buscar os dados que deseja.

Existem algumas informa√ß√µes que s√£o bem comuns de querermos extrair, como os valores das ocorr√™ncias de determinada tag, de um atributo ou mesmo todo o texto da p√°gina.

üëÄ De olho na dica: Ao executar o c√≥digo abaixo, tente executar uma impress√£o por vez, deixando os demais prints comentados enquanto isso, para ter uma melhor visualiza√ß√£o dos retornos. üòâ

````
# import requests
# from bs4 import BeautifulSoup

# url = "https://quotes.toscrape.com"
# page = requests.get(url)
# html_content = page.text

# soup = BeautifulSoup(html_content, "html.parser")

# Imprime todas as ocorr√™ncias da tag "p" da p√°gina ou uma lista vazia,
# caso nenhum elemento corresponda a pesquisa
print(soup.find_all("p"))

# Imprime o elemento com o id especificado ou "None",
# caso nenhum elemento corresponda a pesquisa
print(soup.find(id="quote"))

# Imprime todo o texto da p√°gina
print(soup.get_text())

# Imprime todas as "divs" que possuam a classe "quote" ou uma lista vazia,
# caso nenhum elemento corresponda a pesquisa
print(soup.find_all("div", {"class": "quote"}))
````
Por debaixo dos panos, soup.find_all("p") e soup.find_all(name="p") s√£o a mesma coisa, da mesma forma que soup.find(id="quote") √© o mesmo que soup.find(attrs={"id": "quote"}). Isso se deve ao fato de argumentos nomeados diferentes de name, attrs, recursive, string e limit serem todos colocados no dicion√°rio dentro do par√¢metro attrs.

Para dar uma vis√£o geral do que podemos utilizar e da simplicidade de fazer scraping com o Beautiful Soup, vamos fazer algo similar ao que fizemos no exemplo de Selenium e raspar as informa√ß√µes de uma p√°gina de not√≠cias do site Tecmundo.

````
import requests
from bs4 import BeautifulSoup


# Acessa uma URL e retorna um objeto do Beautiful Soup
def get_html_soup(url):
    page = requests.get(url)
    html_page = page.text

    soup = BeautifulSoup(html_page, "html.parser")
    soup.prettify()
    return soup


# Recebe um objeto soup e retorna informa√ß√µes das not√≠cias de uma p√°gina
def get_page_news(soup):

    # Define uma lista vazia a ser populada com os dados do scraping
    news = []

    # Percorre todos os elementos da tag 'article' com a classe especificada
    for post in soup.find_all(
        "article", {"class": "tec--card tec--card--medium"}
    ):

        # Cria um dicion√°rio para guardar os elementos a cada itera√ß√£o
        item = {}

        # Cria um item chamado tag no dicion√°rio para guardar a tag do post
        # Primeiro pesquisa pela div com a classe espec√≠fica
        # Depois pela tag 'a' dentro dos resultados do primeiro filtro
        # Por fim, traz o resultado da string dentro da tag a
        item["tag"] = post.find("div", {"class": "tec--card__info"}).a.string

        # Mesma l√≥gica da busca anterior
        item["title"] = post.find("h3", {"class": "tec--card__title"}).a.string

        # Parecido com o que foi feito anteriormente, mas dessa vez pega
        # o atributo 'href' dentro da tag 'a'
        item["link"] = post.find("h3", {"class": "tec--card__title"}).a["href"]

        # Mesma l√≥gica da primeira busca, mas trazendo a string dentro da 'div'
        # direto
        item["date"] = post.find(
            "div", {"class": "tec--timestamp__item z--font-semibold"}
        ).string

        # Mesma l√≥gica da busca anterior
        item["time"] = post.find(
            "div", {"class": "z--truncate z-flex-1"}
        ).string

        # Adiciona os itens criado no dicion√°rio √† lista 'news'
        news.append(item)

    return news


print(get_page_news(get_html_soup("https://www.tecmundo.com.br/novidades")))
````
Para fazer a pagina√ß√£o e extrair todas as not√≠cias do site, a l√≥gica √© bem similar a utilizada com o Parsel e o Selenium.

````
# import requests
# from bs4 import BeautifulSoup


# # Acessa uma URL e retorna um objeto do Beautiful Soup
# def get_html_soup(url):
#     page = requests.get(url)
#     html_page = page.text

#     soup = BeautifulSoup(html_page, "html.parser")
#     soup.prettify()
#     return soup


# # Recebe um objeto soup e retorna informa√ß√µes das not√≠cias de uma p√°gina
# def get_page_news(soup):

#     news = []

#     for post in soup.find_all(
#         "article", {"class": "tec--card tec--card--medium"}
#     ):

#         item = {}

#         item["tag"] = post.find("div", {"class": "tec--card__info"}).a.string

#         item["title"] = post.find("h3", {"class": "tec--card__title"}).a.string

#         item["link"] = post.find("h3", {"class": "tec--card__title"}).a["href"]

#         item["date"] = post.find(
#             "div", {"class": "tec--timestamp__item z--font-semibold"}
#         ).string

#         item["time"] = post.find(
#             "div", {"class": "z--truncate z-flex-1"}
#         ).string

#         news.append(item)

#     return news


# Recebe um objeto soup e retorna o link que redireciona
# para a p√°gina seguinte, caso ele exista
def get_next_page(soup_page):
    try:

        # Busca pela tag 'a' com as classes espec√≠ficas do link desejado
        return soup_page.find(
            "a",
            {"class": "tec--btn"},
        )["href"]
    except TypeError:
        return None


# Recebe uma URL e retorna uma lista com todas as not√≠cias do site
def scrape_all(url):
    all_news = []

    # Enquanto a pesquisa pelo link que vai para a p√°gina seguinte existir
    while get_next_page(get_html_soup(url)) is not None:

        # Imprime a URL da p√°gina seguinte
        print(get_next_page(get_html_soup(url)))

        # Adiciona os elementos da lista com as not√≠cias de cada
        # p√°gina na lista 'all_news'
        all_news.extend(get_page_news(get_html_soup(url)))

        # define a p√°gina seguinte como URL para a pr√≥xima itera√ß√£o
        url = get_next_page(get_html_soup(url))

    return all_news


# Vamos come√ßar perto das √∫ltimas p√°ginas pra n√£o ter que fazer a requisi√ß√£o
# do site inteiro
print(scrape_all("https://www.tecmundo.com.br/novidades?page=11030"))
````
As duas ferramentas que voc√™ viu hoje te deram mais possibilidades de fazer raspagem de dados e podem ampliar seu ferramental para ir muito al√©m na sua carreira, explorando outras funcionalidades delas, buscando como integr√°-las ou at√© mesmo procurando outras ferramentas que tragam ainda mais simplicidade e praticidade ao que voc√™ precisa fazer.

Veja agora o v√≠deo que mostra o uso do Beautiful Soup na pr√°tica.'
