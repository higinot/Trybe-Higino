# Introdu√ß√£o

Hoje vamos aprender o que √© raspagem de dados e o que podemos fazer utilizando esta t√©cnica. Vamos ver tamb√©m como fazer requisi√ß√µes web, analisando suas respostas e extraindo dados. Por fim, tamb√©m veremos t√©cnicas para evitar problemas com essa pr√°tica.

## Voc√™ ser√° capaz de:
Realizar requisi√ß√µes web;

Analisar conte√∫dos HTML para extrair dados;

Aplicar t√©cnicas de raspagem para evitar problemas como bloqueio de acesso;

Armazenar os dados obtidos em um banco de dados.

## Por que isso √© importante?
Imagine que voc√™ queira fazer compara√ß√µes de pre√ßos e informa√ß√µes, ou talvez descobrir informa√ß√µes sobre algum assunto. Por√©m todos os dados a respeito do seu assunto de interesse n√£o est√£o estruturados, sendo exibidos somente em uma p√°gina web. Pois √©, uma p√°gina web pode ser √∫til para humanos, mas certamente n√£o √© √∫til para fazermos an√°lises estruturadas.

A raspagem de dados tem sido muito √∫til em trabalhos jornal√≠sticos, fornecendo dados para embasar mat√©rias, mas tamb√©m pode ser √∫til para outros fins, como comparar pre√ßos de produtos com a concorr√™ncia; automatiza√ß√£o de processos ma√ßantes como buscar artigos cient√≠ficos em bases acad√™micas; recupera√ß√£o de documentos em sites jur√≠dicos; analisar perfis de redes sociais; recuperar dados p√∫blicos do governo e muitos outros lugares.

## O que √© raspagem de dados?

Raspagem de dados √© uma t√©cnica computacional para extrair dados provenientes de um servi√ßo ou aplica√ß√£o, estruturando-os em bancos de dados, planilhas ou outros formatos. Essa t√©cnica consiste em extrair dados de sites e transport√°-los para um formato mais simples e male√°vel para que possam ser analisados e cruzados com mais facilidade.

Alguns passos aplicados nesta t√©cnica s√£o: realiza√ß√£o de requisi√ß√µes, an√°lise das respostas e extra√ß√£o dos dados, navega√ß√£o do conte√∫do, limpeza e armazenamento dos dados.

Agora vamos ao ver passo a passo como fazer raspagem de dados, prevenindo erros que podem acontecer. üí™

## Requisi√ß√µes web

A comunica√ß√£o com servidores HTTP e HTTPS se d√° atrav√©s de requisi√ß√µes, nas quais utilizamos o verbo para indicar que tipo de a√ß√£o deve ser tomada para um dado recurso. Devemos informar na requisi√ß√£o em qual recurso estamos atuando e no cabe√ßalho passamos algumas informa√ß√µes que podem ser importantes, como o tipo de resposta aceita ou o tipo de conte√∫do sendo enviado.

O Python possui um pacote para lidar com o protocolo HTTP o urllib, por√©m seu uso √© mais verboso. Por isso vamos utilizar a biblioteca requests, que possui uma interface mais amig√°vel e √© bem difundida na comunidade.

Voc√™ j√° pode instalar a requests dentro de um ambiente virtual, com os seguintes comandos:

````
python3 -m venv .venv && source .venv/bin/activate
python3 -m pip install requests
````

## Rate Limit
Muitas vezes a p√°gina de onde estamos removendo o conte√∫do possui uma limita√ß√£o de quantas requisi√ß√µes podemos enviar em um curto per√≠odo de tempo, evitando assim ataques de nega√ß√£o de servi√ßo.

Isto pode levar a um bloqueio por um curto per√≠odo de tempo ou at√© mesmo banimento de acessar um recurso.

Uma boa pr√°tica √© sempre fazermos uma pausa entre as requisi√ß√µes, ou lote delas, mesmo que o website onde a informa√ß√£o est√° n√£o fa√ßa o bloqueio. Assim, evitamos tirar o site do ar.

````
import requests
import time


# Coloca uma pausa de 6 segundos a cada requisi√ß√£o
# Obs: este site de exemplo tem um rate limit de 10 requisi√ß√µes por minuto
for _ in range(15):
    response = requests.get("https://www.cloudflare.com/rate-limit-test/")
    print(response)
    time.sleep(6)
````

## Timeout
√Ås vezes pedimos um recurso ao servidor, mas caso o nosso tr√°fego de rede esteja lento ou exista um problema interno do servidor, nossa resposta pode demorar ou at√© mesmo ficar travada indefinidamente.

Como garantir, ap√≥s um tempo, que o recurso seja retornado? ü§î

````
import requests


try:
    # recurso demora muito a responder
    response = requests.get("http://httpbin.org/delay/10", timeout=2)
except requests.ReadTimeout:
    # vamos fazer uma nova requisi√ß√£o
    response = requests.get("http://httpbin.org/delay/1", timeout=2)
finally:
    print(response.status_code)
````

## Analisando respostas

Para realizar a extra√ß√£o de dados de um conte√∫do web vamos utilizar uma biblioteca chamada parsel. Ela pode ser instalada com o comando o comando abaixo:

````
python3 -m pip install parsel
````

````
from parsel import Selector
import requests


response = requests.get("http://books.toscrape.com/")
selector = Selector(text=response.text)
print(selector)
# ...


# response = requests.get("http://books.toscrape.com/")
# selector = Selector(text=response.text)

# O t√≠tulo est√° no atributo title em um elemento √¢ncora (<a>)
# Dentro de um h3 em elementos que possuem classe product_pod
titles = selector.css(".product_pod h3 a::attr(title)").getall()
# Estamos utilizando a::attr(title) para capturar somente o valor contido no texto do seletor

# Os pre√ßos est√£o no texto de uma classe price_color
# Que se encontra dentro da classe .product_price
prices = selector.css(".product_price .price_color::text").getall()

# Combinando tudo podemos buscar os produtos
# em em seguida buscar os valores individualmente
for product in selector.css(".product_pod"):
    title = product.css("h3 a::attr(title)").get()
    price = product.css(".price_color::text").get()
    print(title, price)
````

O seletor principal que cont√©m todo o conte√∫do da p√°gina pode ser utilizado em uma busca para encontrar seletores mais espec√≠ficos. Podemos fazer isto utilizando a fun√ß√£o css. Ela recebe um seletor CSS como par√¢metro, embora podemos passar um tipo especial de seletor quando queremos algum valor bem espec√≠fico como o texto ou um atributo.

Ap√≥s definir o seletor, podemos utilizar a fun√ß√£o get para buscar o primeiro seletor/valor que satisfa√ßa aquela busca. A fun√ß√£o getall √© similar, por√©m traz todos os valores que satisfa√ßam aquele seletor.

Assim como temos a fun√ß√£o css que faz a busca por seletores CSS, temos tamb√©m a fun√ß√£o xpath, que faz a busca com base em XPath.`

## Recursos paginados

Tudo certo, temos 20 livros, mas sabemos que na verdade este site possui 1000 livros. Que tal coletarmos todos?!

Navegando um pouco por entre as p√°ginas, percebemos que cada p√°gina possui uma refer√™ncia para a pr√≥xima. Mas, se a URL √© sequencial, por que n√£o jogamos valores at√© a p√°gina avisar que o recurso n√£o foi encontrado? ü§î

Acontece que podemos evitar fazer requisi√ß√µes desnecess√°rias, j√° que a p√°gina nos informa a pr√≥xima.

O que precisamos fazer √© criar um seletor com a p√°gina, extrair os dados, buscar a nova p√°gina e repetir todo o processo, at√© termos navegados em todas as p√°ginas.

At√© a parte da extra√ß√£o n√≥s j√° fizemos, vamos ent√£o dar uma olhada em como buscar a pr√≥xima p√°gina:

Agora que sabemos como recuperar a pr√≥xima p√°gina, podemos iniciar na primeira p√°gina e continuar buscando livros enquanto uma nova p√°gina for encontrada. Cada vez que encontrarmos uma p√°gina, extra√≠mos seus dados e continuamos at√© acabarem as p√°ginas. Ent√£o vamos alterar tudo que t√≠nhamos escrito no arquivo exemplo_scrape.py para o c√≥digo abaixo:

````
from parsel import Selector
import requests


# Define a primeira p√°gina como pr√≥xima a ter seu conte√∫do recuperado
URL_BASE = "http://books.toscrape.com/catalogue/"
next_page_url = 'page-1.html'
while next_page_url:
    # Busca o conte√∫do da pr√≥xima p√°gina
    response = requests.get(URL_BASE + next_page_url)
    selector = Selector(text=response.text)
    # Imprime os produtos de uma determinada p√°gina
    for product in selector.css(".product_pod"):
        title = product.css("h3 a::attr(title)").get()
        price = product.css(".price_color::text").get()
        print(title, price)
    # Descobre qual √© a pr√≥xima p√°gina
    next_page_url = selector.css(".next a::attr(href)").get()
````

## Recursos obtidos √† partir de outro recurso

Agora que estamos coletando todos os livros, que tal coletarmos tamb√©m a descri√ß√£o de cada livro?

O problema √© que a descri√ß√£o s√≥ pode ser acessada navegando at√© a p√°gina espec√≠fica de cada livro.

‚ñ∂Ô∏è O primeiro passo √© investigarmos como descobrir a URL que nos leva at√© a p√°gina de detalhes de um produto. üîç

Com esse seletor em m√£os, precisamos recuperar o atributo href que cont√©m o link para a p√°gina de detalhes do livro. Vamos criar um outro arquivo, apenas para fazer o teste da feature que queremos implementar, depois vamos alterar o c√≥digo do arquivo exemplo_scrape.py para realmente implementar a fun√ß√£o. ‚ö†Ô∏èLembre-se de criar o arquivo no mesmo diret√≥rio que j√° estava utilizando.

````
from parsel import Selector
import requests

URL_BASE = "http://books.toscrape.com/catalogue/"

# Vamos testar com a primeira p√°gina
response = requests.get(URL_BASE + "page-1.html")
selector = Selector(text=response.text)

# Recupera o atributo href do primeiro elemento que combine com o seletor
href = selector.css(".product_pod h3 a::attr(href)").get()
print(href)

# Para obter a url completa, basta adicionar a nossa URL_BASE
print(URL_BASE + href)
````

‚ñ∂Ô∏è A descri√ß√£o do produto est√° uma tag p, ‚Äúirm√£‚Äù de uma tag com id product_description. Isto pode ser expresso atrav√©s do seletor a.

No c√≥digo, precisamos realizar uma nova requisi√ß√£o √† p√°gina de detalhes e depois analisaremos seu conte√∫do em busca da descri√ß√£o do produto. Para isso, vamos alterar todo o conte√∫do novamente, por√©m dessa vez ser√° o conte√∫do do arquivo teste.py:

````
from parsel import Selector
import requests

URL_BASE = "http://books.toscrape.com/catalogue/"

response = requests.get(URL_BASE + "page-1.html")
selector = Selector(text=response.text)

href = selector.css(".product_pod h3 a::attr(href)").get()
detail_page_url = URL_BASE + href

# baixa o conte√∫do da p√°gina de detalhes
detail_response = requests.get(detail_page_url)
detail_selector = Selector(text=detail_response.text)

# recupera a descri√ß√£o do produto
# ~ significa a tag irm√£
description = detail_selector.css("#product_description ~ p::text").get()
print(description)
````

‚ñ∂Ô∏è Por fim, vamos adicionar a l√≥gica para buscar a descri√ß√£o do produto no nosso c√≥digo existente:

````
# from parsel import Selector
# import requests


# URL_BASE = "http://books.toscrape.com/catalogue/"
# Define a primeira p√°gina como pr√≥xima a ter seu conte√∫do recuperado
# next_page_url = 'page-1.html'
while next_page_url:
    # Busca o conte√∫do da pr√≥xima p√°gina
    # response = requests.get(URL_BASE + next_page_url)
    # selector = Selector(text=response.text)
    # Imprime os produtos de uma determinada p√°gina
    for product in selector.css(".product_pod"):
        # Busca e extrai o t√≠tulo e  o pre√ßo
        # title = product.css("h3 a::attr(title)").get()
        # price = product.css(".price_color::text").get()
        # print(title, price)

        # Busca o detalhe de um produto
        detail_href = product.css("h3 a::attr(href)").get()
        detail_page_url = URL_BASE + detail_href

        # Baixa o conte√∫do da p√°gina de detalhes
        detail_response = requests.get(detail_page_url)
        detail_selector = Selector(text=detail_response.text)

        # Extrai a descri√ß√£o do produto
        description = detail_selector.css("#product_description ~ p::text").get()
        print(description)

    # Descobre qual √© a pr√≥xima p√°gina
    # next_page_url = selector.css(".next a::attr(href)").get()
````

## Limpeza de dados

 Estamos extraindo nossos dados, por√©m estes dados cont√©m algumas ‚Äúsujeiras‚Äù que podem atrapalhar em nossas an√°lises. Por exemplo, pare pra olhar o pre√ßo do livro, viu algo diferente? O pre√ßo possui um caractere estranho √Ç¬£26.08 antes do seu valor. E a descri√ß√£o do livro que cont√©m o sufixo ...more.

 Seria conveniente, antes de estruturar e armazenar estes dados, que fiz√©ssemos uma limpeza neles.

 No caso do valor, poder√≠amos utilizar uma express√£o regular para remover os outros caracteres. O padr√£o √© conter um s√≠mbolo de libra, seguido por n√∫meros, ponto para separar casas decimais e dois n√∫meros como casas decimais. Essa express√£o regular pode ser feita da seguinte forma: ¬£\d+\.\d{2}.

 Agora, para utilizar a express√£o regular que fizemos, podemos substituir o m√©todo getall pelo m√©todo re, ou o m√©todo get por re_first. Ambos, al√©m de recuperar valores, aplicar√£o a express√£o regular sobre aquele valor.

 Vamos primeiro fazer essas features, novamente, no arquivo de clean_crawer.py para vermos funcionando. Depois vamos implement√°-las no arquivo exemplo_scrape.py:

 ````
 from parsel import Selector
import requests


response = requests.get("http://books.toscrape.com/")
selector = Selector(text=response.text)
# Extrai todos os pre√ßos da primeira p√°gina
prices = selector.css(".product_price .price_color::text").re(r"¬£\d+\.\d{2}")
print(prices)
````

 J√° para o caso do sufixo ...more, poder√≠amos utilizar fatiamento para remov√™-lo. Mas antes √© importante verificarmos se o conte√∫do possui o sufixo, evitando assim perda de conte√∫do de forma acidental. Vamos ver como isso funciona no arquivo clean_crawer_2.py:

 ````
 from parsel import Selector
import requests


response = requests.get("http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html")
selector = Selector(text=response.text)

# Extrai a descri√ß√£o
description = selector.css("#product_description ~ p::text").get()
print(description)

# "Fatiamos" a descri√ß√£o removendo o sufixo
suffix = "...more"
if description.endswith(suffix):
    description = description[:-len(suffix)]
print(description)
````

 Alguns outros exemplos de ‚Äúsujeiras‚Äù s√£o valores que cont√©m tabula√ß√µes, quebras de linha ou conte√∫do al√©m do esperado.

Agora vamos implementar essas funcionalidades no nosso arquivo exemplo_scrape.py:

````
from parsel import Selector
import requests


# URL_BASE = "http://books.toscrape.com/catalogue/"
# Define a primeira p√°gina como pr√≥xima a ter seu conte√∫do recuperado
# next_page_url = 'page-1.html'
# while next_page_url:
    # Busca o conte√∫do da pr√≥xima p√°gina
    # response = requests.get(URL_BASE + next_page_url)
    # selector = Selector(text=response.text)
    # Imprime os produtos de uma determinada p√°gina
    # for product in selector.css(".product_pod"):
        # Busca e extrai o t√≠tulo e  o pre√ßo
        # title = product.css("h3 a::attr(title)").get()
        price = product.css(".product_price .price_color::text").re(r"¬£\d+\.\d{2}")
        # print(title, price)

        # Busca o detalhe de um produto
        # detail_href = product.css("h3 a::attr(href)").get()
        # detail_page_url = URL_BASE + detail_href

        # Baixa o conte√∫do da p√°gina de detalhes
        # detail_response = requests.get(detail_page_url)
        # detail_selector = Selector(text=detail_response.text)

        # Extrai a descri√ß√£o do produto
        # description = detail_selector.css("#product_description ~ p::text").get()
        suffix = "...more"
        if description.endswith(suffix):
            description = description[:-len(suffix)]
        # print(description)

    # Descobre qual √© a pr√≥xima p√°gina
    # next_page_url = selector.css(".next a::attr(href)").get()
````


üëÄ De olho na dica: Strings em Python possuem v√°rios m√©todos para ajudarem nesta limpeza, como por exemplo, o strip. Para ler a documenta√ß√£o e procurar esses m√©todos, execute help(str) no seu terminal interativo.

## Fatiamento

Estruturas de dados do tipo sequ√™ncia, como listas, tuplas e strings, podem ter seus elementos acessados atrav√©s de um √≠ndice:

````
# Recupera o primeiro elemento
[1, 2, 3][0]  # Sa√≠da: 1
````

Podemos tamb√©m definir dois √≠ndices que ser√£o o valor inicial e final de uma subsequ√™ncia da estrutura. Ou tr√™s valores, sendo o √∫ltimo o tamanho do passo que daremos ao percorrer a subsequ√™ncia:

````
# Quando n√£o incluso o valor inicial, iniciaremos do √≠ndice 0
# e do √≠ndice 2 em diante, os elementos n√£o s√£o inclu√≠dos
(1, 2, 3, 4)[:2]  # Sa√≠da: (1, 2)

# Quando omitimos o valor final
# o fatiamento ocorre at√© o fim da sequ√™ncia
(1, 2, 3, 4)[1:]  # Sa√≠da: (2, 3, 4)

# V√° do √≠ndice 3 at√© o 7.
# O item no √≠ndice 7 n√£o √© inclu√≠do
"palavra"[3:7]  # Sa√≠da: "avra"

# Come√ßando do elemento de √≠ndice 1, v√° at√© o √∫ltimo,
# saltando de 2 em 2
[1, 2, 3, 4][1::2]  # Sa√≠da: [2, 4]
````

Chamamos esta opera√ß√£o de fatiamento. Ela √© muito utilizada para obtermos uma subsequ√™ncia de uma sequ√™ncia.

‚ö†Ô∏è√Ä partir da vers√£o 3.9 do Python, teremos uma fun√ß√£o chamada removesuffix, que √© equivalente √† palavra[:-len(suffix)].

## Banco de Dados

Agora que temos nossos dados, precisamos armazenar esta informa√ß√£o. Para isto utilizaremos o MongoDB que, como j√° estudamos, √© um banco de dados de documentos, que armazena dados em formato JSON (BSON). Precisaremos de uma biblioteca para nos comunicarmos com o sistema de gerenciamento do banco de dados, e a mais popular e robusta √© a pymongo. Podemos instal√°-la com o comando:

Lembre-se que para testar o c√≥digo abaixo voc√™ deve criar um ambiente virtual e instalar o pymongo conforme √© ensinado abaixo.

````
python3 -m venv .venv && source .venv/bin/activate
python3 -m pip install pymongo
````

Ap√≥s a instala√ß√£o vamos ver como podemos realizar a escrita e leitura neste banco de dados. O primeiro passo √© criar uma conex√£o com o banco de dados e isto pode ser feito da seguinte maneira:

‚ö†Ô∏è Lembre-se que o MongoDB deve estar preparado para ser acessado do ‚Äúoutro lado‚Äù dessa opera√ß√£o!.

````
from pymongo import MongoClient

client = MongoClient()
# o banco de dados catalogue ser√° criado se n√£o existir
db = client.catalogue
# a cole√ß√£o books ser√° criada se n√£o existir
students = db.books
client.close()  # fecha a conex√£o com o banco de dados
````

Para adicionarmos documentos √† nossa cole√ß√£o utilizamos o m√©todo insert_one:

````
from pymongo import MongoClient

client = MongoClient()
db = client.catalogue
# book representa um dado obtido na raspagem
book = {
    "title": "A Light in the Attic",
}
document_id = db.books.insert_one(book).inserted_id
print(document_id)
client.close()  # fecha a conex√£o com o banco de dados
````

Quando um documento √© inserido, um _id √∫nico √© gerado e retornado. Tamb√©m podemos fazer inser√ß√£o de m√∫ltiplos documentos de uma vez da seguinte forma:

````
from pymongo import MongoClient

client = MongoClient()
db = client.catalogue
documents = [
    {
        "title": "A Light in the Attic",
    },
    {
        "title": "Tipping the Velvet",
    },
    {
        "title": "Soumission",
    },
]
db.books.insert_many(documents)
client.close()  # fecha a conex√£o com o banco de dados
````

Buscas podem ser feitas utilizando os m√©todos find ou find_one:

````
from pymongo import MongoClient

client = MongoClient()
db = client.catalogue
# busca um documento da cole√ß√£o, sem filtros
print(db.books.find_one())
# busca utilizando filtros
for book in db.books.find({"title": {"$regex": "t"}}):
    print(book["title"])
client.close()  # fecha a conex√£o com o banco de dados
````

O nosso cliente √© um gerenciador de contexto (with), logo podemos utiliz√°-lo como tal, evitando problemas com o fechamento da conex√£o com o banco de dados:

````
from pymongo import MongoClient


with MongoClient() as client:
    db = client.catalogue
    for book in db.books.find({"title": {"$regex": "t"}}):
        print(book["title"])
````

## B√¥nus

### Scrapy
üï∑ Uma excelente e poderosa ferramenta para raspagem de dados √© a Scrapy. Ela possui, em sua implementa√ß√£o, todos os mecanismos citados anteriormente e outros recursos adicionais.

Vale a pena dar uma olhadinha! üòâ


